{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdaece30",
   "metadata": {},
   "source": [
    "# MODELOS EVOLUTIVOS E DE REFORÇO\n",
    "\n",
    "O aluno Yan(118982) desenvolveu o algoritmo evolutivo, e o aluno Leonardo(233548) desenvolveu o algoritmo de Aprendizado por Reforço.\n",
    "\n",
    " A seguir, mostramos os algoritmos com um pequeno texto introdutório mostrando suas premissas, e depois apresentamos o código. Isso para cada algoritmo.\n",
    "\n",
    "# 1. Algoritmo Genético\n",
    "\n",
    "## 1.1 Ficha-Resumo:\n",
    "\n",
    "• <b>Modelo adotado:</b> Algotirmo Genético Modificado;<br>\n",
    "• <b>Gene:</b> Lista com 5 floats entre 0 e 100; <br>\n",
    "• <b>Chromossomo:</b> 1000 genes em um vetor; <br>\n",
    "• <b>Função Fitness:</b> Score do jogo; <br>\n",
    "• <b>Tamanho da População:</b> testamos com 20 e 500 indivíduos; <br>\n",
    "• <b>População inicial:</b> Escolha alheatória com distribuição uniforme; <br>\n",
    "• <b>Critério de Parada: </b> 100 gerações;<br>\n",
    "• <b>Técnica de Seleção:</b> Torneio com 4 indivíduos; <br>\n",
    "• <b>Técnica de Crossover:</b> Double point com a troca de 100 genes; <br>\n",
    "• <b>Técnica de Mutação:</b> Swap; <br>\n",
    "• <b>Método de Replacement:</b>Steady State (5 melhores da pop anterior e n-5 melhores da pop intermediaria) ; <br>\n",
    "• <b>Taxa de Mutação:</b> 10%; <br>\n",
    "• <b>Taxa de Crossover:</b> 80%; <br>\n",
    "\n",
    "## 1.2 Explicação:\n",
    "\n",
    "\n",
    " \n",
    "### 1.2.1 Gene:\n",
    "Abaixo, apresentamos um fluxograma para um estado, com um gene de um agente para este estado, contendo uma lista de 5 pesos que variam de 0 a 100. Cada peso irá multiplicar o output de uma função do agente que retorna um dicionário com as direções. Cada direção conterá um valor calculado por uma idéia de distância heurística que definimos e que explicaremos a seguir. A ideia é que os pesos definam a importância do output (as direções) de cada função para a tomada de descisão do agente para a próxima direção.\n",
    "\n",
    "Após definir a próxima direção, o agente checa se essa direção é possível. Se for, ele move-se à ela, se não, escolhemos a segunda direção ótima. E novamente checamos se é possível realizá-la, e continuamos o processo até encontrar uma direção possível. Após mover-se para um novo estado, o agente recebe um novo gene, ou seja, um novo conjunto de pesos para ponderar o output das funções, e assim o processo se repete até que o jogo acabe, ou que o limite de 1000 ações (1000 genes que definem os pesos para a tomada de decisões do agente em cada estado) seja atingido. Se atingirmos esse limite, o agente pára e espera algum fantasma o encontrar.\n",
    "<img height=\"500px\" class=\"center-block\" src=\"assets/pacman.jpeg\">\n",
    "\n",
    "\n",
    "\n",
    "### 1.2.2 Distância Heurística do agente para os fantasmas:\n",
    "\n",
    "Como exemplo, imaginemos que nosso agente se encontra na posição (3,3) do grid e existe um fantasma na posição (6,5), como mostrado na Figura 1. Nossa função de distância heurística checa o maior eixo do fantasma em relação ao pacman e vota para ir na distância oposta ao fantasma. Se tivermos dois fantasmas, como na figura 2, fazemos o mesmo processo, calculamos o maior eixo de distância do fantasma ao pacman e votamos para as direções opostas.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"col-md-6\" markdown=\"1\">\n",
    "   Figura 1. Pacman e um fantasma\n",
    "  <img height=\"500px\" class=\"center-block\" src=\"assets/pac1.jpg\">\n",
    "  </div>\n",
    "  <div class=\"col-md-6\" markdown=\"1\">\n",
    "   Figura 2. Votando na direção para dois fantasmas\n",
    "  <img height=\"500px\" class=\"center-block\" src=\"assets/pac2.jpg\">\n",
    "  </div>    \n",
    "   <div class=\"col-md-12\" markdown=\"1\">\n",
    "        Caso tivermos um terceiro fantasma cuja distância dos eixos são iguais, então votamos para as duas direções opostas ao fantasma, como podemos conferir na imagem abaixo. No exemplo, nossa função de distância para todos os fantasmas retornaria um dicionário com as seguintes direções <i>{'North':0,'East':1,'South':2,'West':1},</i> que nada mais é do que somar cada voto para cada direção.\n",
    "       Desse dicionário, multiplicamos o peso para esta função, definido pela posição na lista de pesos do gene.\n",
    "       <br><br>\n",
    "       Figura 3. Votação final fantasmas\n",
    "   <img height=\"400px\" class=\"center-block\" src=\"assets/pac3.jpg\">\n",
    "   </div>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "\n",
    "### 1.2.3 Distância Heurística do agente por comida:\n",
    "Já para comida, nosso agente utiliza uma estratégia um pouco diferente. Primeiro, consideramos que o agente  estará sempre na posição (0,0). Além disso, ele se preocupa apenas com as comidas que estão dentro de um quadrado, que se inicia com lado de tamanho 4 unidades, como demonstrado na figura 4. Se não encontramos nenhuma comida, o lado do quadrado aumenta em duas unidades, como na figura 5. Caso ainda nosso agente não encontre comida, o processo se repete até 15 tentativas.\n",
    "\n",
    "Ao encontrar comida, calculamos a mesma distância heurística que utilizamos para os fantasmas, porém agora nosso algoritmo vota para a direção a favor da comida, e não na direção oposta, como era feito com os fantasmas.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<div class=\"row\">\n",
    " <div class=\"col-md-5\" markdown=\"1\">\n",
    "Figura 4. Quadrado de busca inicializado com lado 4.\n",
    "  \n",
    "  </div>\n",
    "  <div class=\"col-md-1\" markdown=\"1\">\n",
    "      \n",
    "  </div>\n",
    "\n",
    "  <div class=\"col-md-5\" markdown=\"1\">\n",
    " Figura 5. Expansão do quadrado e cálculo das distâncias\n",
    "  </div>\n",
    " \n",
    "    \n",
    "    \n",
    "  <div class=\"col-md-5\" markdown=\"1\">\n",
    "\n",
    "  <img height=\"500px\" class=\"center-block\" src=\"assets/pac4.jpg\">\n",
    "  </div>\n",
    "  <div class=\"col-md-1\" markdown=\"1\">\n",
    "      \n",
    "  </div>\n",
    "\n",
    "  <div class=\"col-md-5\" markdown=\"1\">\n",
    "  <img height=\"500px\" class=\"center-block\" src=\"assets/pac5.jpg\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### 1.2.3 Definindo a próxima posição baseado em todas as funções:\n",
    "É importante salientar que o quadrado só serve para comida. Ou seja, mesmo que um fantasma não se encontre dentro do quadrado, a direção oposta à sua posição entrará na votação. Na figura 6 apresentamos a votação para todas as funções que realizamos. É importante lembrar que para cada função (distância da comida mais próxima, distância para todas as comidas dentro do quadrado, distância de todos os fantasmas, distância do fantasma mais próximo e a próxima jogada que apresente o maior score momentâneo), essa votação das direções será ponderada por um peso que o gene traz para cada função. \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 6. Votação final\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/pac6.jpg\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.3 Análise dos Resultados:\n",
    "\n",
    "### 1.3.1 SmallClassic:\n",
    "Nossos resultados não foram satisfatórios, pois nosso algoritmo não convergiu. Abaixo mostramos o comportamento do melhor, pior e indivíduo médio por geração.\n",
    "<br><br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 7. Scores por geração SmallClassic. <br>\n",
    " Legenda: melhor indivíduo por geração (linha laranja), pior indivíduo de cada geração (linha azul) e a média de cada geração (linha verde).\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/line_erro.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "Percebemos que nossos indivíduos não melhoraram o score ao longo das gerações, apresentavam apeas alguma variação aleatória em torno de um valor médio, algo que se assemelha a um comportamento de ruído branco de séries temporais (mas no caso do ruído branco, o valor médio é zero, o que não é nosso caso). \n",
    "<br><br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 8. Distribuição dos scores dos melhores indivíduos de cada geração. <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/dist_erro.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "Ao analisarmos a distribuição do score do melhor indivíduo de cada geração, percebemos uma distribuição com assimetria positiva (à direita), o que demonstra que os melhores scores, na verdade, são mais outliers do que tendência.\n",
    "\n",
    "### 1.3.1 MediumClassic:\n",
    "Para o Medium Grid, obtivemos resultados similares. Rodamos para 100 gerações, mas já é suficiente para notar que não há melhora. Podemos ver que a distribuição dos scores dos melhores indivíduos apresentam uma similaridade.\n",
    "<br><br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 9. Ditribuição dos scores do melhor indivíduo por geração no MediumClassic. <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/line_erro_medium.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 1.3.1 OriginalClassic:\n",
    "Novamente, para o layout original do jogo os mesmo resultados.\n",
    "<br><br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 10. Scores por geração OriginalClassic. <br>\n",
    "    Legenda: melhor indivíduo por geração (linha laranja), pior indivíduo de cada geração (linha azul) e a média de cada geração (linha verde).\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/line_erro_large.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.4 O que deu errado?\n",
    "Ao ver essa falta de convergência, tentamos mudar o tipo de distribuição de seleção dos pesos. Estávamos selecionando valores de 0 a 100 sob uma distribuição uniforme. Tentamos selecionar valores de uma distribuição normal com média zero e variância 1, e também não obtivemos sucesso. Talvez se considerarmos que esses pesos na verdade como probabilidade que varia de 0 a 1 e cuja somatória dos pesos fosse igual a 1. O problema é que isso dificultaria amutação, já que haveria a restrição de que a somatória dos pesos sempre fosse igual a um. Por tornar o código mais complexo e aumentar o tempo de implementação, decidimos seguir em frente com a parte dois do trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afcc9c",
   "metadata": {},
   "source": [
    "## 1.4 CÓDIGO:\n",
    "### 1.4.1 Definição do agente do jogo Pacman para Algoritmo genético\n",
    "\n",
    "Baseamos nossa escolha de agente como um pacman que decide o próximo passo a ser dado ao analisar em seu estado atual qual a posição de todos os fantasmas, do fantasma mais próximo, das comidas dentro de um raio e da comida mais próxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pacman import GameState,readCommand\n",
    "from collections import Counter\n",
    "from game import Agent\n",
    "from game import Directions\n",
    "from util import manhattanDistance\n",
    "from heuristic import heuristicDistance\n",
    "import seaborn as sns\n",
    "import pacman\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GAAgent(Agent):\n",
    "    '''O melhor agente da Terra que recebe pesos para a relevância de funções de busca, como\n",
    "    a posição da melhor comida, do fantasma mais próximo, etc, para calcular o próximo passo do Agente'''\n",
    "    \n",
    "    def __init__(self,weights):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def registerInitialState(self, state):\n",
    "        return    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def getAction(self,state):\n",
    "        \n",
    "        def dirComida(state,weight1,weight2,lado_quadrado):\n",
    "            '''Função que recebe dois pesos e um número. Ela verifica por comida num quadrado em torno do pacman.\n",
    "            A função recebe o tamanho do lado do quadrado. Ela retorna a direção em que mais comidas estão nesse\n",
    "            quadrado (Oeste,Leste,Norte, Sul), ponderada pelo peso 1 e a direção da comida mais próxima ponderada\n",
    "            pelo peso 2.'''\n",
    "            east=west=north=south=0 #inicializando as variaveis para contar as direções mais frequentes\n",
    "            best_east=best_west=best_north=best_south=0 #inicializando as variaveis para as direções\n",
    "                                                        #da comida mais próxima\n",
    "            near_x=near_y=0\n",
    "            near_food=10000\n",
    "            #Cria o quadrado ladoxlado em volta do pacman para checar se tem comida e qual a direção da comida\n",
    "            for eixo_x in range(int(-(lado_quadrado/2)),int((lado_quadrado/2)+1)):\n",
    "                for eixo_y in range(int(-(lado_quadrado/2)),int((lado_quadrado/2)+1)):\n",
    "                    try:\n",
    "                        #Se tem muro, vai para outro quadrado\n",
    "                        if state.hasWall(state.getPacmanPosition()[0]+eixo_x,state.getPacmanPosition()[1]+eixo_y):\n",
    "                            continue\n",
    "                        else:\n",
    "                            #Se não tem muro, atualiza o x e y do quadrado\n",
    "                            x = state.getPacmanPosition()[0]+eixo_x\n",
    "                            y = state.getPacmanPosition()[1]+eixo_y\n",
    "                            has_food = state.getFood()[x][y]\n",
    "                            #Verifica se tem comida no quadrado e adiciona a direção mais forte(sul,norte,leste,oeste)\n",
    "                            if has_food:\n",
    "                                #Ve se é a comida mais proxima, se for irá armazenar as direções para o weight 2\n",
    "                                #Utilizamos a distância de Manhattan pra ver qual comida é a mais próxima\n",
    "                                dist=manhattanDistance(state.getPacmanPosition(),[x,y])\n",
    "                                if dist<near_food:\n",
    "                                    near_food=dist\n",
    "                                    near_x= x\n",
    "                                    near_y= y\n",
    "                                    #Aqui vereficamos o maior eixo e o sinal. Se for,por exemplo, eixo x e sinal \n",
    "                                    #positivo, será direção oeste. Se for o mesmo eixo e sinal contrário, será \n",
    "                                    #direção este. Do mesmo modo se o eixo y for o maior, será norte se for\n",
    "                                    #positivo e sul se for negativo\n",
    "                                    if abs(eixo_x)>abs(eixo_y) and eixo_x<0:\n",
    "                                        best_west = 0\n",
    "                                        best_west += 1\n",
    "                                    elif abs(eixo_x)>abs(eixo_y) and eixo_x>0:\n",
    "                                        best_east=0\n",
    "                                        best_east += 1\n",
    "                                    elif abs(eixo_y)>abs(eixo_x) and eixo_y<0:\n",
    "                                        best_south=0\n",
    "                                        best_south += 1\n",
    "                                    elif abs(eixo_y)>abs(eixo_x) and eixo_y>0:\n",
    "                                        best_north=0\n",
    "                                        best_north += 1\n",
    "                                    elif abs(eixo_y)==abs(eixo_x) and eixo_y>0 and eixo_x>0:\n",
    "                                        best_north=best_east=0\n",
    "                                        best_north += 1\n",
    "                                        best_east += 1\n",
    "                                    elif abs(eixo_y)==abs(eixo_x) and eixo_y>0 and eixo_x<0:\n",
    "                                        best_north=best_west=0\n",
    "                                        best_north += 1\n",
    "                                        best_west += 1\n",
    "                                    elif abs(eixo_y)==abs(eixo_x) and eixo_y<0 and eixo_x>0:\n",
    "                                        best_south=best_east=0\n",
    "                                        best_south += 1\n",
    "                                        best_east += 1\n",
    "                                    elif abs(eixo_y)==abs(eixo_x) and eixo_y<0 and eixo_x<0:\n",
    "                                        best_south=best_west=0\n",
    "                                        best_south += 1\n",
    "                                        best_west += 1\n",
    "                                #Depois de vermos se é a comida mais próxima, agora checamos a mesma coisa para\n",
    "                                #as demais comidas. Mesmo se for a comida mais próxima, ela entrará na conta da\n",
    "                                #comida total.\n",
    "                                if abs(eixo_x)>abs(eixo_y) and eixo_x<0:\n",
    "                                    west += 1\n",
    "                                elif abs(eixo_x)>abs(eixo_y) and eixo_x>0:\n",
    "                                    east += 1\n",
    "                                elif abs(eixo_y)>abs(eixo_x) and eixo_y<0:\n",
    "                                    south += 1\n",
    "                                elif abs(eixo_y)>abs(eixo_x) and eixo_y>0:\n",
    "                                    north += 1\n",
    "                                elif abs(eixo_y)==abs(eixo_x) and eixo_y>0 and eixo_x>0:\n",
    "                                    north += 1\n",
    "                                    east += 1\n",
    "                                elif abs(eixo_y)==abs(eixo_x) and eixo_y>0 and eixo_x<0:\n",
    "                                    north += 1\n",
    "                                    west += 1\n",
    "                                elif abs(eixo_y)==abs(eixo_x) and eixo_y<0 and eixo_x>0:\n",
    "                                    south += 1\n",
    "                                    east += 1\n",
    "                                elif abs(eixo_y)==abs(eixo_x) and eixo_y<0 and eixo_x<0:\n",
    "                                    south += 1\n",
    "                                    west += 1                            \n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "            #Agora que fizemos a contagem das direções, a função retorna\n",
    "            #dois dicionários, um para a comida mais próxima e outro para o total de comidas.\n",
    "            # Cada dicionário contém todas as direções como keys e a contagem das direções, já ponderada\n",
    "            #pelo reespectivos pesos, 1 e 2 (Peso 1 para a comida total e peso 2 para a comida mais próxima).\n",
    "            actions_count_all={'East':weight1*east,'West':weight1*west,'North':weight1*north,'South':weight1*south}\n",
    "            \n",
    "            actions_count_near={'East':weight2*best_east,'West':weight2*best_west,\n",
    "                                'North':weight2*best_north,'South':weight2*best_south}\n",
    "            return actions_count_all , actions_count_near\n",
    "        \n",
    "        def futureBestState(state,legal,weight):\n",
    "            dic = {'East':0,'West':0,'North':0,'South':0}\n",
    "            score_list = [0,0,0,0]\n",
    "            for action in dic.keys():\n",
    "                if action in legal:\n",
    "                    score = state.generatePacmanSuccessor(action).getScore()\n",
    "                    dic[action]=score*weight\n",
    "            return dic\n",
    "        def dirNearGhost(state,weight):\n",
    "            '''Função que recebe um peso, verifica pelo fantasma mais perto e retorna direção oposta ponderada\n",
    "            pelo peso recebido.'''\n",
    "            \n",
    "            east=west=north=south=0 #inicializando as variaveis para contar as direções mais frequentes\n",
    "            numGhost = state.getNumAgents() #Número de fantasmas\n",
    "            near_dist = 10000 #Estabelecendo um valor de distância alto para ser substituido\n",
    "            #função que busca o fantasma mais perto, baseado na distância de manhattan\n",
    "            for ghost in range(1,numGhost):\n",
    "                dist=manhattanDistance(state.getPacmanPosition(),state.getGhostPosition(ghost))\n",
    "                if dist<near_dist:\n",
    "                    near_dist=dist\n",
    "                    near_ghost=ghost\n",
    "            #Selecionamos agora a direção do fantasma mais proximo\n",
    "            x,y=heuristicDistance(state.getPacmanPosition(),state.getGhostPosition(near_ghost))\n",
    "            #Aqui inevertemos a melhor direção, para o pacman ir para uma direção oposta do fantasma\n",
    "            if abs(x)>abs(y):\n",
    "                if x>0:\n",
    "                    east += 1\n",
    "                else:\n",
    "                    west += 1\n",
    "                    \n",
    "            elif abs(x)==abs(y):\n",
    "                if x>0:\n",
    "                    east += 1\n",
    "                    north += 1\n",
    "                else:\n",
    "                    west += 1\n",
    "                    south += 1\n",
    "            else:\n",
    "                if y>0:\n",
    "                        north += 1\n",
    "                else:\n",
    "                        south += 1\n",
    "            #Retornamos um dicionário com todas as direções como key e a contagem, já ponderada pelo peso,\n",
    "            #como values                         \n",
    "            actions_count={'East':weight*east,'West':weight*west,'North':weight*north,'South':weight*south}\n",
    "            return actions_count\n",
    "         \n",
    "        def dirAllGhost(state,weight):\n",
    "            '''Função que recebe um peso, verifica os fantasmas e retorna a direção oposta da somatória dos fantasmas\n",
    "            ponderada pelo peso passado na função.'''\n",
    "           \n",
    "            east=west=north=south=0 #inicializando as variaveis para contar as direções mais frequentes\n",
    "            numGhost = state.getNumAgents() #Número de fantasmas. Sabemos que o pacman é contado aqui, mas não importa\n",
    "                                            # porque na função range() utilizada abaixo começamos no 1, e não no zero\n",
    "\n",
    "            for ghost in range(1,numGhost):\n",
    "                x,y=heuristicDistance(state.getPacmanPosition(),state.getGhostPosition(ghost))\n",
    "                #Aqui inevertemos a melhor direção, para o pacman ir para uma direção oposta ao do fantasma\n",
    "                if abs(x)>abs(y):\n",
    "                    if x>0:\n",
    "                        east += 1\n",
    "                    else:\n",
    "                        west += 1\n",
    "                    \n",
    "                elif abs(x)==abs(y):\n",
    "                    if x>0:\n",
    "                        east += 1\n",
    "                        north += 1\n",
    "                    else:\n",
    "                        west += 1\n",
    "                        south += 1\n",
    "                else:\n",
    "                    if y>0:\n",
    "                        north += 1\n",
    "                    else:\n",
    "                        south += 1\n",
    "            #Retornamos um dicionário com todas as direções como key e a contagem, já ponderada pelo peso,\n",
    "            #como values            \n",
    "            actions_count={'East':weight*east,'West':weight*west,'North':weight*north,'South':weight*south}\n",
    "            return actions_count\n",
    "        \n",
    "        #Agora iniciamos a função getAction, retirando um dos cromossomos do indivíduo (cada\n",
    "        #passo dado pelo indivíduo no jogo, ponderada pelos pesos). Ou seja, o gene contém a lista de pesos\n",
    "        #para cada uma das funções estabelecidas: peso 1 dado para a melhor direção para comidas, peso dois \n",
    "        #para a melhor direção para comida mais próxima, peso três para a direção oposta\n",
    "        # de todos os fantasmas e direção oposta do fantasma mais próximo.\n",
    "        try:\n",
    "            gene=self.weights.pop(0) #Retira um gene da lista de passos\n",
    "        except:\n",
    "            return Directions.STOP\n",
    "        legal = state.getLegalPacmanActions() #Verifica as ações legais do agente\n",
    "        tentativas=0 #Inicializa a variável de tentativas para achar a melhor direção dado os pesos\n",
    "        lado_quadrado=2 #Inicializa o tamanho do lado do quadrado do pacman para a busca da comida\n",
    "        food = state.getNumFood()\n",
    "        while tentativas<15 and food > 10:#Tenta 15 vezes encontrar alguma direção ou ate faltar 10 comidas\n",
    "            tentativas+=1 #Vamos contando o número de tentativas\n",
    "            lado_quadrado+=2 #A cada tentativa, aumentamos o quadrado de busca por comida\n",
    "            #Vemos o dicionário de ações para a melhor comida e para todas as comidas\n",
    "            all_food_best_dir, near_food_best_dir = dirComida(state,int(gene[0]),int(gene[1]),lado_quadrado)\n",
    "            #Vemos o dicionário de ações para o fantasma mais próximo e todos os fantasmas\n",
    "            all_ghost_best_dir = dirAllGhost(state,int(gene[2]))\n",
    "            near_ghost_best_dir = dirNearGhost(state,int(gene[3]))\n",
    "            best_future_action = futureBestState(state,legal,int(gene[4]))\n",
    "            final = dict(Counter(all_food_best_dir) + Counter(near_food_best_dir) + \n",
    "                     Counter(all_ghost_best_dir)+\n",
    "                     Counter(near_ghost_best_dir)+\n",
    "                     Counter(best_future_action))\n",
    "            final = dict(sorted(final.items(), key=lambda item: item[1],reverse=True))\n",
    "            for i in range(len(final)):\n",
    "                melhor_direcao = list(final.keys())[i]\n",
    "                if melhor_direcao in legal:\n",
    "                    return melhor_direcao\n",
    "        #Se nenhuma dessas tentativas derem certo, para evitar erro, retornamos uma direção aleatória dentre\n",
    "        # as direções legais dos agentes\n",
    "        all_food_best_dir, near_food_best_dir = dirComida(state,int(gene[0]),int(gene[1]),lado_quadrado)\n",
    "        best_future_action = futureBestState(state,legal,int(gene[4]))\n",
    "        final = dict(Counter(near_food_best_dir) + Counter(best_future_action))\n",
    "        final = dict(sorted(final.items(), key=lambda item: item[1],reverse=True))\n",
    "        for i in range(len(final)):\n",
    "            melhor_direcao = list(final.keys())[i]\n",
    "            if melhor_direcao in legal:\n",
    "                return melhor_direcao\n",
    "        final = dict(sorted(near_food_best_dir.items(), key=lambda item: item[1],reverse=True))\n",
    "        if list(final.keys())[0] in legal:\n",
    "            return list(final.keys())[0]\n",
    "        return random.choice(legal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98936f71",
   "metadata": {},
   "source": [
    "### 1.4.2 Funções para o algoritmo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c530d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGene():\n",
    "    '''Função que retorna um gene. Ela gera uma lista com 5 números de 0 a 100 criados aleatoriamente'''\n",
    "    gene=[]\n",
    "    for weight in range(0,5):\n",
    "        gene.append(random.uniform(0, 100))\n",
    "    return gene\n",
    "\n",
    "def generateIndividuo():\n",
    "    '''Função que retorna um indivíduo. Ela gera um indivíduo com 1000 passos, valor suficiente para resolver\n",
    "    até o layout mais complexo do pacman.'''\n",
    "    individuo=[]\n",
    "    for jogadas in range(0,1000):\n",
    "        individuo.append(generateGene())\n",
    "    return individuo\n",
    "\n",
    "def generatePopulation(size):\n",
    "    '''Função que recebe o layout do jogo e um tamanho e cria uma população de indivíduos. Ela\n",
    "    retorna duas listas, a população e os scores de cada indivíduo. O score na posição 0 da lista será do \n",
    "    indivíduo da população na posição 0.'''\n",
    "    population=[]\n",
    "    for i in range(0,size):\n",
    "        individuo = generateIndividuo()\n",
    "        individuo_temp=individuo #Passamos para uma variável temporária pois o .pop(0)do getAction\n",
    "                                    #remove os passos dados dos indivíduos\n",
    "        population.append(individuo)\n",
    "    return population\n",
    "\n",
    "def pacmanAvarageScore(individuo,num_tentativas,flag):\n",
    "    '''Função que recebe o indivíduo, a quantidade de jogos que ele irá realizar e o layout do jogo.\n",
    "    Ela retorna o score médio da quantidade de jogos realizados para esse indivíduo.'''\n",
    "    tentativas_ind=[]\n",
    "    layouts=['originalClassic']\n",
    "    for layout in layouts:\n",
    "        for tentativas in range(0,num_tentativas):\n",
    "            if flag:\n",
    "                args = ['--layout',layout,'--pacman','DumbAgent','-q']\n",
    "            else:\n",
    "                args = ['--layout',layout,'--pacman','DumbAgent']\n",
    "            args_list = readCommand(args)\n",
    "            test = GAAgent(individuo)\n",
    "            score= pacman.runGames(pacman=test,layout=args_list['layout'],ghosts=args_list['ghosts'],display=args_list['display'],\n",
    "                           numGames=args_list['numGames'],record=args_list['record'])\n",
    "            tentativas_ind.append(score)\n",
    "    return np.mean(tentativas_ind)\n",
    "\n",
    "def evaluatePopulation(population,flag):\n",
    "    '''Função que recebe o layout do jogo e um tamanho e cria uma população de indivíduos. Ela\n",
    "    retorna duas listas, a população e os scores de cada indivíduo. O score na posição 0 da lista será do \n",
    "    indivíduo da população na posição 0.'''\n",
    "    scores=[]\n",
    "    for i in range(0,len(population)):\n",
    "        print('individuo ',i+1)\n",
    "        individuo = population[i]\n",
    "        individuo_temp=deepcopy(individuo) #Passamos para uma variável temporária pois o .pop(0)do getAction\n",
    "                                    #remove os passos dados dos indivíduos\n",
    "        avg_score = pacmanAvarageScore(individuo_temp,5,flag)\n",
    "        scores.append(avg_score)\n",
    "    return scores\n",
    "\n",
    "def mutation(population,rate):\n",
    "    '''Função que recebe uma população e uma taxa e define uma porcentagem da população para realizar mutação\n",
    "    de cromossomos nos seus indivíduos da essa taxa recebida'''\n",
    "    for selected in range(int(len(population)*rate)): #Seleciona uma quantidade x de indivíduos, dado a rate\n",
    "        position_individual = random.randint(0,len(population)-1) #seleciona um indivíduo aleatório na população\n",
    "        position_cromossom1 = random.randint(0,len(population[position_individual])-1)#Seleciona um cromossomo aleatório\n",
    "        position_cromossom2 = random.randint(0,len(population[position_individual])-1)#Seleciona outro cromossomo aleatório\n",
    "        crom1 = population[position_individual][position_cromossom1]\n",
    "        crom2 = population[position_individual][position_cromossom2]\n",
    "        \n",
    "        population[position_individual][position_cromossom1] = crom2\n",
    "        population[position_individual][position_cromossom2] = crom1\n",
    "    return population\n",
    "\n",
    "def torneio(pairs,tournment_size):\n",
    "    participantes_torneio=[]\n",
    "    for number in range(tournment_size):\n",
    "        position_individual = random.randint(0,len(pairs)-1)\n",
    "        individuo = pairs[position_individual]\n",
    "        participantes_torneio.append(individuo)\n",
    "    participantes_torneio.sort(reverse=True, key=lambda li: li[0])\n",
    "    return participantes_torneio[0]\n",
    "\n",
    "def crossover(pairs,range_cromossomos,aleatorio):\n",
    "    '''Função que recebe um pair [score, população] e uma taxa e um range de valores e define uma porcentagem da população\n",
    "    para realizar crossover desse range de valores (recebido pela função) para os cromossomos dos indivíduos'''\n",
    "    novos_pares=[]\n",
    "    for i in range(int((len(pairs)*0.8)/2)): #Para 80% dosindivíduos de individuos na populacao\n",
    "        \n",
    "        individual1 = torneio(pairs,4) #seleciona um indivíduo 1 aleatório na população\n",
    "        individual2 = torneio(pairs,4) #seleciona um indivíduo 2 aleatório na população\n",
    "        #Apos selecionar as posições dos indivíduos, selecionamos o range de seus primeiros cromossomos     \n",
    "        crom1_list = individual1[1][aleatorio:aleatorio+range_cromossomos]\n",
    "        crom2_list = individual2[1][aleatorio:aleatorio+range_cromossomos]\n",
    "            \n",
    "        individual1[1][aleatorio:aleatorio+range_cromossomos] = crom2_list\n",
    "        individual2[1][aleatorio:aleatorio+range_cromossomos] = crom1_list\n",
    "        novos_pares.append(individual1)\n",
    "        novos_pares.append(individual2)\n",
    "    population = [el[1] for el in novos_pares]\n",
    "    return population\n",
    "\n",
    "def makePairs(scores,population):\n",
    "    lista=[]\n",
    "    for i in range(len(population)):\n",
    "        item=[scores[i], population[i]]\n",
    "        lista.append(item)\n",
    "    lista.sort(reverse=True, key=lambda li: li[0])\n",
    "    return lista\n",
    "\n",
    "def mergePopulationAndScores(pairs,pairs_linha):\n",
    "    pop = pairs[:5] + pairs_linha[:-5]\n",
    "    population = [el[1] for el in pop]\n",
    "    scores = [el[0] for el in pop]\n",
    "    return population, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63e8f5",
   "metadata": {},
   "source": [
    "### 1.4.3 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70273985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}\n",
    "#genetic program procedure\n",
    "t = 0\n",
    "\n",
    "# Initialize_Population P(0)\n",
    "size=20\n",
    "P = generatePopulation(size)\n",
    "# While not (wall_condition) do\n",
    "while t <= 100: \n",
    "    print('GERAÇÃO',t)\n",
    "    # Evaluate_Population P(t)\n",
    "    print('pais:')\n",
    "    scores = evaluatePopulation(P,flag=True)\n",
    "    pairs= makePairs(scores,P)\n",
    "    #Duplicationof the best 25% individuals of P(t)\n",
    "    pairs_duplication = pairs[:int(len(pairs)*0.2)]\n",
    "    Pduplication = [el[1] for el in pairs_duplication]\n",
    "    # P_cross = crossover 80% de P(t) de 100 genes\n",
    "    aleatorio = np.random.randint(0,600)\n",
    "    Pcrossover = crossover(pairs,100,aleatorio)\n",
    "    #P'(t) = duplication + cross\n",
    "    \n",
    "    Plinha = Pcrossover + Pduplication\n",
    "    \n",
    "    Plinha = mutation(Plinha,0.1)\n",
    "    # Evaluate_Population P’\n",
    "    print('filhos:')\n",
    "    scores_linha  = evaluatePopulation(Plinha,flag=True)\n",
    "    pairs_linha = makePairs(scores_linha,Plinha)\n",
    "    \n",
    "    # P(t+1) = Select_s survivors P(t) from P '\n",
    "    P, scores_final = mergePopulationAndScores(pairs,pairs_linha)\n",
    "    dic[t] = scores_final\n",
    "    t = t + 1\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cdb9b",
   "metadata": {},
   "source": [
    "### 1.4.4 Análise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores=[]\n",
    "worst_scores=[]\n",
    "mean_score=[]\n",
    "for i in dic.keys():\n",
    "    best_scores.append(max(dic[i]))\n",
    "    worst_scores.append(min(dic[i]))\n",
    "    mean_score.append(sum(dic[i]) / len(dic[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "df['best'] = best_scores\n",
    "df['worst'] = worst_scores\n",
    "df['mean'] = mean_score\n",
    "df['generations'] = list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8456ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# add the plots for each dataframe\n",
    "sns.lineplot(x='generations', y='worst', data=df)\n",
    "sns.lineplot(x='generations', y='best', data=df)\n",
    "sns.lineplot(x='generations', y='mean', data=df)\n",
    "ax.set(ylabel='Avarage Game Score', xlabel='Generations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65647c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# add the plots for each dataframe\n",
    "sns.regplot(x='generations', y='worst', data=df, fit_reg=True)\n",
    "sns.regplot(x='generations', y='best', data=df, fit_reg=True)\n",
    "ax.set(ylabel='y', xlabel='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb76459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('100_geracoes_small_mutation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee4419",
   "metadata": {},
   "source": [
    "# 2. Algoritmo de aprendizado por reforço\n",
    "\n",
    "## 2.1 Ficha-Resumo:\n",
    "\n",
    "• <b>Ambiente:</b> Estocástico (dado o aspecto aleatório dos fantasmas); <br>\n",
    "• <b>Modelo Adotado:</b> Q-learning ; <br>\n",
    "• <b>Número de episódios de treino:</b> 2000 episódios; <br>\n",
    "• <b>Reward: </b> Pontuação recebida pela ação adotada;<br>\n",
    "• <b>Critério de Parada: </b> Número de episódios;<br>\n",
    "• <b>Valor do Learning Rate ($\\alpha$) =</b> 1; <br>\n",
    "• <b>Fator de Desconto ($\\gamma$) =</b> 0.8; <br>\n",
    "• <b>Taxa de exploração ($\\epsilon$) =</b> 0.05; <br>\n",
    "\n",
    "## 2.2 Explicação:\n",
    "Rodamos nosso agente para 2000 episódios, computando o score do jogo, e considerando o reward como o score recebido pela ação adotada. Abaixo, mostramos nosso agente para os três tipos de layout. Percebemos que nosso agente obteve o melhor reward médio no mediumGrid, e que a tendência é que o reward se estabilize depois uns 500 episódios.\n",
    "\n",
    "### 2.2.1 smallClassic:\n",
    "\n",
    "O Reward médio tem um grande incremento nos primeiros episódios e se estabiliza em torno de 100. Já o score, percebemos que nosso agente no teste com 10 episódios ganhou alguns mas perdeu outros.\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 1. Rewards médios <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/qlearning_reward_small.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 2. Score teste <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/line_train_score_small.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### 2.2.2 mediumClassic:\n",
    "\n",
    "O Reward médio tem um grande incremento nos primeiros episódios e se estabiliza em torno de 220. Já o score, percebemos que nosso agente no teste com 10 episódios ganhou todos.\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 3. Rewards médios <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/qlearning_reward_medium.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br><br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 4. Score teste <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/line_train_score_medium.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 2.2.3 originalClassic:\n",
    "\n",
    "O Reward médio tem um grande incremento nos primeiros episódios e se estabiliza em torno de -60. Já o score, percebemos que nosso agente no teste com 10 episódios ganhou todos.\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 5. Rewards médios <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/qlearning_reward_original.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br><br>\n",
    "<div class=\"col-md-12\" markdown=\"1\">\n",
    "   Figura 6. Score teste <br>\n",
    "   <img height=\"200px\" class=\"center-block\" src=\"assets/line_train_score_original.png\">\n",
    "   </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.3 Código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e148ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirNearFood(pos, food, walls):\n",
    "        '''Função que retorna a posição da comida mais próxima'''\n",
    "        matrix = [(pos[0], pos[1], 0)]\n",
    "        neigh = set()\n",
    "        while matrix:\n",
    "\n",
    "            pos_x, pos_y, dist = matrix.pop(0)\n",
    "            if (pos_x, pos_y) not in neigh:\n",
    "                neigh.add((pos_x, pos_y))\n",
    "\n",
    "                if food[pos_x][pos_y]:\n",
    "                    return dist\n",
    "\n",
    "                nbrs = Actions.getLegalNeighbors((pos_x, pos_y), walls)\n",
    "                for nbr_x, nbr_y in nbrs:\n",
    "                    matrix.append((nbr_x, nbr_y, dist+1))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import *\n",
    "import pacman\n",
    "from pacman import GameState,readCommand\n",
    "import random,util,math,time\n",
    "import util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lista=[]\n",
    "class QAgent():\n",
    "\n",
    "    def __init__(self, alpha=1.0, epsilon=0.05, gamma=0.8, numTraining = 2000):\n",
    "        # alpha       - learning rate\n",
    "        # epsilon     - exploration rate\n",
    "        # gamma       - discount factor\n",
    "        self.episodesSoFar = 0\n",
    "        self.accumTrainRewards = 0.0\n",
    "        self.accumTestRewards = 0.0\n",
    "        self.numTraining = int(numTraining)\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.alpha = float(alpha)\n",
    "        self.discount = float(gamma)\n",
    "        self.QValues = util.Counter()\n",
    "        self.weights = util.Counter()\n",
    "        \n",
    "\n",
    "\n",
    "    def stopEpisode(self):\n",
    "        if self.episodesSoFar < self.numTraining:\n",
    "            self.accumTrainRewards += self.episodeRewards\n",
    "        else:\n",
    "            self.accumTestRewards += self.episodeRewards\n",
    "        self.episodesSoFar += 1\n",
    "        if self.episodesSoFar >= self.numTraining:\n",
    "            self.epsilon = 0.0    \n",
    "            self.alpha = 0.0     \n",
    "\n",
    "    def computeQValues(self, state):\n",
    "        values = [self.getQValue(state, action) for action in state.getLegalActions()]\n",
    "        if (values):\n",
    "            return max(values)\n",
    "        else:\n",
    "            return 0.0 \n",
    "\n",
    "    def computeActionFromQValues(self, state):\n",
    "        legal_actions = state.getLegalActions()\n",
    "\n",
    "        value = self.computeQValues(state)\n",
    "        for action in legal_actions:\n",
    "            if (value == self.getQValue(state, action)):\n",
    "                return action\n",
    "\n",
    "    def getFeatures(self, state, action):\n",
    "        food = state.getFood()\n",
    "        walls = state.getWalls()\n",
    "        ghosts = state.getGhostPositions()\n",
    "        capsule = state.getCapsules()\n",
    "\n",
    "        features = util.Counter()\n",
    "        #features[\"bias\"] = 1.0\n",
    "\n",
    "        x, y = state.getPacmanPosition()\n",
    "        dx, dy = Actions.directionToVector(action)\n",
    "        next_x, next_y = int(x + dx), int(y + dy)\n",
    "        next_2x, next_2y = int(x + 2*dx), int(y + 2*dy)\n",
    "\n",
    "        features[\"g1passo\"] = sum((next_x, next_y) in Actions.getLegalNeighbors(g, walls) for g in ghosts)\n",
    "        features[\"g2passo\"] = sum((next_2x, next_2y) in Actions.getLegalNeighbors(g, walls) for g in ghosts)\n",
    "\n",
    "        if not features[\"g1passo\"] and food[next_x][next_y]:\n",
    "            features[\"eats-food\"] = 1.0\n",
    "\n",
    "        dist = dirNearFood((next_x, next_y), food, walls)\n",
    "        if dist is not None:\n",
    "            features[\"closest-food\"] = float(dist) / (walls.width * walls.height)\n",
    "\n",
    "        features.divideAll(10.0)\n",
    "        return features\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        features = self.getFeatures(state,action)\n",
    "        QValue = 0.0\n",
    "\n",
    "        for feature in features:\n",
    "            QValue += self.weights[feature] * features[feature]\n",
    "\n",
    "        return QValue\n",
    "\n",
    "    def update(self, state, action, nextState, reward):\n",
    "        QValue = 0\n",
    "        difference = reward + (self.discount * self.computeQValues(nextState) - self.getQValue(state, action))\n",
    "        features = self.getFeatures(state, action)\n",
    "\n",
    "        for feature in features:\n",
    "            self.weights[feature] += self.alpha * features[feature] * difference\n",
    "\n",
    "    def registerInitialState(self, state):\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.episodeRewards = 0.0\n",
    "\n",
    "    def test(self, state):\n",
    "        deltaReward = state.getScore() - self.lastState.getScore()\n",
    "        self.episodeRewards += deltaReward   \n",
    "        self.update(self.lastState, self.lastAction, state, deltaReward)\n",
    "        self.stopEpisode()\n",
    "\n",
    "        # Make sure we have this var\n",
    "        if not 'episodeStartTime' in self.__dict__:\n",
    "            self.episodeStartTime = time.time()\n",
    "        if not 'lastWindowAccumRewards' in self.__dict__:\n",
    "            self.lastWindowAccumRewards = 0.0\n",
    "        self.lastWindowAccumRewards += state.getScore()\n",
    "\n",
    "        numEpis = 10\n",
    "        if self.episodesSoFar % numEpis == 0:\n",
    "            print('Status:')\n",
    "            windowAvg = self.lastWindowAccumRewards / float(numEpis)\n",
    "            \n",
    "            trainAvg = self.accumTrainRewards / float(self.episodesSoFar)\n",
    "            print ('\\tRewards médio: %.2f' % (\n",
    "                    trainAvg))\n",
    "            self.lastWindowAccumRewards = 0.0\n",
    "            lista.append(trainAvg)\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            msg = 'Treino terminado'\n",
    "            print ('%s\\n%s' % (msg,'-' * len(msg)))\n",
    "            print ('\\tRewards Médio: %.2f' % (\n",
    "                    trainAvg))\n",
    "            lista.append(trainAvg)\n",
    "    def getAction(self, state):\n",
    "        legalActions = state.getLegalActions()\n",
    "        action = None\n",
    "\n",
    "        if (util.flipCoin(self.epsilon)):\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            action = self.computeActionFromQValues(state)\n",
    "\n",
    "        self.lastState = state\n",
    "        self.lastAction = action\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['--layout','mediumClassic','--pacman','DumbAgent','-n','2010','-q']\n",
    "args_list = readCommand(args)\n",
    "test = QAgent()\n",
    "scores= pacman.runGames(pacman=test,layout=args_list['layout'],ghosts=args_list['ghosts'],display=args_list['display'],\n",
    "            numGames=args_list['numGames'],record=args_list['record'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65157d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lista)\n",
    "plt.title('Reward médio de 10 episódios mediumGrid')\n",
    "plt.xlabel('trainig/10')\n",
    "plt.ylabel('Reward Médio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores[-10:])\n",
    "plt.title('Score de 10 episódios teste')\n",
    "plt.xlabel('episodio')\n",
    "plt.ylabel('Score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
